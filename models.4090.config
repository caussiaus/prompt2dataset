# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# RTX 4090 Optimized Model Configuration
# GPU: 24GB VRAM | System: 32GB RAM
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

# ═══════════════════════════════════════════════════
# PRIMARY MODELS (Always loaded in VRAM)
# ═══════════════════════════════════════════════════

# Vision & OCR (Best available)
qwen3-vl                # 4.7GB - Superior vision/OCR
llama3.2-vision         # 7.9GB - Alternative vision

# General LLM (Large model - GPU can handle it)
qwen2.5:32b             # 18GB - Excellent quality
# OR use 70B if you want maximum quality (slower):
# llama3.1:70b          # 40GB - Needs model offloading

# Embeddings
bge-m3                  # 2.2GB - Best multilingual embeddings

# ═══════════════════════════════════════════════════
# SECONDARY MODELS (Load on demand)
# ═══════════════════════════════════════════════════

# Fast alternative LLM
llama3.1                # 4.7GB - Fast, good quality

# Specialized reasoning
deepseek-r1:14b         # 8.9GB - Complex reasoning tasks

# Code generation
codellama:13b           # 7.3GB - Code tasks
# OR
qwen3-coder             # 4.7GB - Faster code model

# ═══════════════════════════════════════════════════
# OPTIONAL MODELS (Uncomment if needed)
# ═══════════════════════════════════════════════════

# Even larger LLM (if you're doing complex tasks)
# mixtral:8x7b          # 26GB - Mixture of experts

# Specialized vision
# bakllava              # 4.6GB - Vision focused

# Multilingual
# aya:35b               # 20GB - 101 languages

# ═══════════════════════════════════════════════════
# TOTAL SIZE CALCULATION
# ═══════════════════════════════════════════════════
# Primary models: ~32GB disk, ~24GB VRAM when loaded
# All models: ~80GB disk
# 
# With 24GB VRAM, you can keep 2-3 large models loaded
# Ollama will automatically swap models as needed
# ═══════════════════════════════════════════════════

# ═══════════════════════════════════════════════════
# RECOMMENDED CONFIGURATION FOR YOUR SETUP:
# ═══════════════════════════════════════════════════
# Keep loaded in VRAM simultaneously:
# - qwen3-vl (vision)
# - qwen2.5:32b (main LLM) 
# - bge-m3 (embeddings)
# = Total: ~25GB VRAM (perfect fit for 4090!)
# ═══════════════════════════════════════════════════
